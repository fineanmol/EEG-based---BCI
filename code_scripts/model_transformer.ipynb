{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af15ed9a-6dfb-4656-8cf2-9960ad2af249",
   "metadata": {},
   "source": [
    "# Title: Transformer Model (MSC/MPC Features)\n",
    "\n",
    "This notebook demonstrates how to train and evaluate a CNN_LSTM model on EEG data (processed via MSC/MPC features). It includes steps for data loading, augmentation, dimensionality reduction (PCA), training, confusion matrix visualization, per-class bar charts, and an accuracy comparison with/without PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ff7934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data=pd.read_csv('EEG_data_subject4', sep=\"\\t\") #loading data from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8e130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imagined=pd.read_csv('EEG_data_subject3',sep=\"\\t\")\n",
    "df_imagined.drop([\"Time\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89e85a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imagined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c0cc8b-82a6-473e-9975-6cc24740f142",
   "metadata": {},
   "source": [
    "# Loading EEG data\n",
    "This section loads the EEG datasets, merges them if necessary, and displays initial information about the data structures. We then define some variables and dictionaries related to the classification labels (words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c2f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inner=pd.read_csv('EEG_data_subject4',sep=\"\\t\")\n",
    "df_inner.drop([\"Time\"],axis=1,inplace=True)\n",
    "df_combined=pd.concat([df_imagined,df_inner])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e97cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.array(df_imagined)\n",
    "data=data.transpose()\n",
    "data.shape\n",
    "y_array=np.load(\"labels_word_list.npy\")\n",
    "# y_array=np.array(list(y_array)*2) --for combined data\n",
    "y_array[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e6b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dict={'date':0,'goose':1,'spruce':2,'knight':3,'juice':4,'moose':5,'night':6,'queen':7,'berry':8,'hedgehog':9,'water':10,'daughter':11,'gooseberry':12,'waterfowl':13,'wilderness':14,'relative':15,'watermelon':16,'caterpillar':17,'environment':18,'ambassador':19}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cee206",
   "metadata": {},
   "source": [
    "# 2) Data Epoching / Segmenting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683589f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We break the EEG data into epochs of 256 samples, you are effectively segmenting the data into 1-second intervals.\n",
    "print(\"Epoching data...\")\n",
    "n=int(data.shape[1]/256)\n",
    "epoched_data=[]\n",
    "for i in range(n):\n",
    "    epoched_data.append(data[:,256*i:256*(i+1)])\n",
    "epoched_data=np.array(epoched_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d7b45f-041d-4d03-93d3-20a44805ac7c",
   "metadata": {},
   "source": [
    "# Shuffling / random permutation\n",
    "Ensures randomization of the epochs and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84efaaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply random permutation\n",
    "p=np.random.RandomState(seed=42).permutation(len(y_array))\n",
    "epoched_data=epoched_data[p]\n",
    "y_array=y_array[p]\n",
    "epoched_data.shape  #(num_epochs,num_channels,epoch_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9df3131",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "Creates additional “sub-epochs” by sliding windows within each original epoch.\n",
    "\n",
    "Here, we create overlapping sub‐epochs (128 samples each, with a certain overlap) to artificially increase the number of training examples. Then we replicate labels accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c361110",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data=[]\n",
    "n=epoched_data.shape[0]\n",
    "length=128\n",
    "overlap=32\n",
    "for i in range(n):\n",
    "    for j in range(0,length+1,overlap):\n",
    "        augmented_data.append(epoched_data[i][:,j:j+length])\n",
    "augmented_data=np.array(augmented_data)\n",
    "augmented_data.shape #(num_epochs,num_channels,epoch_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2d3879",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoched_data=augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa51e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_array=[]\n",
    "x=int(augmented_data.shape[0]/5000) #/1000 for 5000\n",
    "for word in y_array:\n",
    "    for i in range(x):\n",
    "        new_array.append(word)\n",
    "y_array=np.array(new_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1244705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoched_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db8e097-a79d-4300-a8f8-56fdf7b03a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust Label for Augmented Data. i.e Replicate each label the appropriate number of times\n",
    "# You must replicate each label 5 times\n",
    "\n",
    "augmented_labels = []\n",
    "for label in y_array:\n",
    "    for _ in range(5):  # or however many sub-trials you have per original\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "y_array = np.array(augmented_labels)\n",
    "print(\"y_array.shape:\", y_array.shape)  # should now be (5000,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f091798a",
   "metadata": {},
   "source": [
    "# Creating continuous data from epoched data\n",
    "\n",
    "We reshape/concatenate the epochs so we can perform subsequent filtering (alpha, beta, gamma extraction) using MNE functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d175e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating continuous data from epoched data\n",
    "continuous_data=[]\n",
    "trials=epoched_data.shape[0]\n",
    "channels=epoched_data.shape[1]\n",
    "sample_size=epoched_data.shape[2]\n",
    "for i in range(channels):\n",
    "    continuous_data.append(epoched_data[:,i].reshape(trials*sample_size))\n",
    "continuous_data=np.array(continuous_data)\n",
    "continuous_data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2323f18c",
   "metadata": {},
   "source": [
    "# **Extracting Alpha, Beta, Gamma Bands with MNE from continuous data**\n",
    "\n",
    "We apply notch filtering at 60 Hz, downsample, and specifically extract alpha, beta, and gamma bands for each channel, then re‐epoch them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf402e23-e50b-4e2f-9d72-94db59ce8ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mne --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bb5da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup MNE for filtering\n",
    "import mne\n",
    "sfreq=128\n",
    "ch_names=[\"F3\",\"FC5\",\"AF3\",\"F7\",\"T7\",\"P7\",\"O1\",\"O2\",\"P8\",\"T8\",\"F8\",\"AF4\",\"FC6\",\"F4\"] \n",
    "info=mne.create_info(ch_names,sfreq=sfreq)\n",
    "raw=mne.io.RawArray(continuous_data,info)\n",
    "raw.plot(scalings = 'auto');\n",
    "print(\"Filtering EEG bands...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1d4d89",
   "metadata": {},
   "source": [
    "# Filtering and downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df976f6",
   "metadata": {},
   "source": [
    "Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b6fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notch filter at 60 Hz\n",
    "raw.notch_filter(60,picks='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1cdeff",
   "metadata": {},
   "source": [
    "Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29182b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) downsampling\n",
    "raw.resample(120, npad='auto')\n",
    "# Notice that the max plotted frequency is 60, the nyquist rate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60b93e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the alpha, beta, gamma from EEG\n",
    "# Uses MNE‐Python’s FIR filter to isolate each band.\n",
    "\n",
    "alpha_continuous=mne.filter.filter_data(continuous_data,128,8,12)\n",
    "beta_continuous=mne.filter.filter_data(continuous_data,128,12,30)\n",
    "gamma_continuous=mne.filter.filter_data(continuous_data,128,30,50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcf881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"alpha_continuous shape:\", alpha_continuous.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a349b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoching all the frequency bands\n",
    "\n",
    "trial_duration=epoched_data.shape[2] #trial duration\n",
    "n=epoched_data.shape[0]\n",
    "alpha_epoched=[]\n",
    "beta_epoched=[]\n",
    "gamma_epoched=[]\n",
    "for i in range(n):\n",
    "    alpha_epoched.append(alpha_continuous[:,i*trial_duration:(i+1)*trial_duration])\n",
    "    beta_epoched.append(beta_continuous[:,i*trial_duration:(i+1)*trial_duration])\n",
    "    gamma_epoched.append(gamma_continuous[:,i*trial_duration:(i+1)*trial_duration])\n",
    "alpha_epoched=np.array(alpha_epoched)\n",
    "beta_epoched=np.array(beta_epoched)\n",
    "gamma_epoched=np.array(gamma_epoched)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"alpha_epoched shape:\", alpha_epoched.shape)  #(num_epochs,num_channels,epoch_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1c0dcb",
   "metadata": {},
   "source": [
    "# **Feature Extraction (MPC & MSC)**\n",
    "\n",
    "We compute Mean Phase Coherence (MPC) and Magnitude Squared Coherence (MSC) for alpha, beta, gamma bands, and assemble them into a feature vector. This process can be computationally heavy, so a progress indicator (`print(i,end=' ')`) is shown.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc1cd0b",
   "metadata": {},
   "source": [
    "In this second technique, the features,  **mean phase coherence (MPC)** is extracted along with **magnitude-squared coherence (MSC)** from the augmented data \\citep{panachakel2021decoding}. MPC between two EEG channels is described as a measure of their phase synchronisation. The mean phase coherence (MPC) between two EEG signals with instantaneous phase difference $\n",
    "    \\phi(t)=\\phi_1(t)-\\phi_2(t)\n",
    "$ can be estimated via,\n",
    "\n",
    "$\n",
    "    \\lambda = \\frac{1}{N}\\Bigg|\\sum_{n=0}^{N-1}e^{j(\\hat{\\phi}_i(n))}\\Bigg|\n",
    "$\n",
    "where $\n",
    "    {(\\hat{\\phi}_i(n))}_{n=0}^{N-1}\n",
    "$ is the estimation of $\n",
    "    \\phi(t)\n",
    "$,\n",
    "where N is the number of samples,and the instantaneous phases are computed using Hilbert transform.\n",
    "Where as, if a pair of signals are in spectral domain, MSC computes the linear relationship between them. Hamming window is used for this process. Let the auto-spectral densities and the cross-spectral density  of $\n",
    "    x(t)\n",
    "$ and $\n",
    "    y(t)\n",
    "$  be denoted by $\n",
    "    P_{xx}(f)\n",
    "$, $\n",
    "    P_{yy}(f)\n",
    "$  and $\n",
    "    P_{xy}(f)\n",
    "$ respectively at frequency f. The MSC between them is given by:\n",
    "\n",
    "$\n",
    "    \\gamma _{xy}^{2} (f) = {{\\left\\vert {P_{xy} (f)} \\right\\vert^{2} } \\over {P_{xx} (f)P_{yy} (f)}} \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a94d8",
   "metadata": {},
   "source": [
    "**MSC (Magnitude Squared Coherence)**:  Measures the strength of correlation between two EEG signals in the frequency domain.\n",
    "\n",
    "**MPC (Mean Phase Coherence)**:  Measures how well two EEG signals stay in phase over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908e3d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculation of hilbert array from augmented array\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal import welch\n",
    "from scipy.signal import csd\n",
    "#Defining magnitude squared coherence\n",
    "import numpy as np\n",
    "from scipy.signal import welch\n",
    "from scipy.signal import csd,coherence\n",
    "\n",
    "def msc(arr1,arr2):\n",
    "    #Magnitude‐Squared Coherence (MSC)\n",
    "    #Measures the linear correlation in the frequency domain.\n",
    "    msc=coherence(arr1,arr2,fs=128,window=\"hamm\",nperseg=8)[1]\n",
    "    return np.mean(msc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913d3859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining mean phase coherence\n",
    "from scipy.signal import hilbert\n",
    "import numpy as np\n",
    "def mpc(arr1,arr2):\n",
    "    imag_1=np.imag(hilbert(arr1))\n",
    "    imag_2=np.imag(hilbert(arr2))\n",
    "    phase_1=np.arctan(np.divide(imag_1, arr1, out=np.zeros_like(imag_1), where=arr1!=0))\n",
    "    phase_2=np.arctan(np.divide(imag_2, arr2, out=np.zeros_like(imag_2), where=arr2!=0))\n",
    "    phase_diff=(phase_1-phase_2)\n",
    "    mpc=np.linalg.norm(np.sum(np.exp(1j*phase_diff)))/len(arr1)\n",
    "    return mpc\n",
    "print(\"Calculating coherence matrices ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a986d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=alpha_epoched.shape[0]\n",
    "m=alpha_epoched.shape[1]\n",
    "l=alpha_epoched.shape[2]\n",
    "\n",
    "mpc_alpha=np.zeros([n,m,m])\n",
    "mpc_beta=np.zeros([n,m,m])\n",
    "mpc_gamma=np.zeros([n,m,m])\n",
    "\n",
    "msc_alpha=np.zeros([n,m,m])\n",
    "msc_beta=np.zeros([n,m,m])\n",
    "msc_gamma=np.zeros([n,m,m])\n",
    "\n",
    "print(mpc_gamma.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9536dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(n):\n",
    "    for j in range(m):\n",
    "        for k in range(m):\n",
    "            mpc_alpha[i][j][k]=mpc(alpha_epoched[i][j],alpha_epoched[i][k])\n",
    "            msc_alpha[i][j][k]=msc(alpha_epoched[i][j],alpha_epoched[i][k])\n",
    "            \n",
    "            mpc_beta[i][j][k]=mpc(beta_epoched[i][j],beta_epoched[i][k])\n",
    "            msc_beta[i][j][k]=msc(beta_epoched[i][j],beta_epoched[i][k])\n",
    "            \n",
    "            mpc_gamma[i][j][k]=mpc(gamma_epoched[i][j],gamma_epoched[i][k])\n",
    "            msc_gamma[i][j][k]=msc(gamma_epoched[i][j],gamma_epoched[i][k])\n",
    "    print(i,end=' ')\n",
    "\n",
    "print(\"\\nMPC/MSC calculations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc80be3-2594-4682-9bcf-fe1f6b7fa14f",
   "metadata": {},
   "source": [
    "# **Creating the Feature Vector**\n",
    "\n",
    "We pack MPC (above the diagonal) and MSC (below the diagonal) for alpha, beta, gamma into a single 3D array, then flatten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cec0ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpc_alpha_copy=mpc_alpha\n",
    "mpc_beta_copy=mpc_beta\n",
    "mpc_gamma_copy=mpc_gamma\n",
    "\n",
    "msc_alpha_copy=msc_alpha\n",
    "msc_beta_copy=msc_beta\n",
    "msc_gamma_copy=msc_gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0709b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "msc_alpha_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38042f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpc_alpha_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7850fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "files=[mpc_alpha,mpc_beta,mpc_gamma,msc_alpha,msc_beta,msc_gamma]\n",
    "file_names=[\"mpc_alpha\",\"mpc_beta\",\"mpc_gamma\",\"msc_alpha\",\"msc_beta\",\"msc_gamma\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b8684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "part=\"01\"\n",
    "file_type=\"inner\"\n",
    "aug_type=\"non-aug\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52a847d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#saving the files\n",
    "for i in range(len(files)):\n",
    "    path=''+part+'/'+part+'_'+file_type+'/'+aug_type+'/'+file_names[i]+'.npy'\n",
    "    np.save(path,files[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be8cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the files\n",
    "for i in range(len(files)):\n",
    "    path=''+part+'/'+part+'_'+file_type+'/'+aug_type+'/'+file_names[i]+'.npy'\n",
    "    files[i]=np.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a5a532",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpc_alpha=files[0]\n",
    "mpc_beta=files[1]\n",
    "mpc_gamma=files[2]\n",
    "\n",
    "msc_alpha=files[3]\n",
    "msc_beta=files[4]\n",
    "msc_gamma=files[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bbb3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "msc_alpha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7744ff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpc_alpha.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd1dba2",
   "metadata": {},
   "source": [
    "This section is used to create the feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dda6cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_1=msc_alpha.shape[0]\n",
    "m_1=msc_alpha.shape[1]\n",
    "x_array_2=np.zeros([n_1,m_1,m_1,3])\n",
    "print(\"mpc_alpha shape:\", mpc_alpha.shape)\n",
    "print(\"mpc_beta shape:\", mpc_beta.shape)\n",
    "print(\"mpc_gamma shape:\", mpc_gamma.shape)\n",
    "\n",
    "print(\"msc_alpha shape:\", msc_alpha.shape)\n",
    "print(\"msc_beta shape:\", msc_beta.shape)\n",
    "print(\"msc_gamma shape:\", msc_gamma.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c73e926",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_1):\n",
    "    for j in range(m_1):\n",
    "        for k in range(m_1):\n",
    "            if j<k:\n",
    "                x_array_2[i][j][k]=[mpc_alpha[i][j][k],mpc_beta[i][j][k],mpc_gamma[i][j][k]]\n",
    "            elif j>k:\n",
    "                x_array_2[i][j][k]=[msc_alpha[i][j][k],msc_beta[i][j][k],msc_gamma[i][j][k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546024e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array_cnn=x_array_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f370325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=x_array_2.reshape(n_1,m_1*m_1*3)\n",
    "a=a[a!=0.0]\n",
    "a.shape\n",
    "x_array_2=a.reshape(n_1,m_1*m_1*3-m_1*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96809d3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_array_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeca8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(''+part+'/'+part+'_'+file_type+'_'+\"X\",x_array_2)\n",
    "np.save(''+part+'/'+part+'_'+file_type+'_'+\"Y\",y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d562f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array=np.load(''+part+'/'+part+'_'+file_type+'_'+\"X\"+\".npy\")\n",
    "y_array=np.load(''+part+'/'+part+'_'+file_type+'_'+\"Y\"+\".npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76edb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array = x_array_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c9bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f4ba4f-eb34-4d23-a370-38ac590ac407",
   "metadata": {},
   "source": [
    "# Reshaping x_array for a “Single Time-Step” Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7f33e4-1183-4301-97a1-c6219a057b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N, F = x_array.shape\n",
    "seq_len = 1\n",
    "input_dim = F\n",
    "\n",
    "# Reshape from (N, F) -> (N, 1, F)\n",
    "X_seq = x_array.reshape(N, seq_len, input_dim)\n",
    "\n",
    "print(\"X_seq shape:\", X_seq.shape)  # (N, 1, F)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a45565",
   "metadata": {},
   "source": [
    "In order to reduce the dimension of the feature vector Principal component analysis (PCA) was used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af57e3f",
   "metadata": {},
   "source": [
    "# **Principal component analysis (PCA) / Standardization**\n",
    "Standard scaling is also used to zero‐mean/unit‐variance the features.\n",
    "We apply PCA to capture 95% variance. This yields the “with PCA” scenario. We will compare it to a “without PCA” scenario later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14779617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "x_array_without_pca = x_array\n",
    "pca=PCA(.95)\n",
    "pca.fit(x_array)\n",
    "x_array=pca.transform(x_array)\n",
    "x_array_with_pca=x_array\n",
    "print('x_array_without_pca',x_array_without_pca.shape,\"[x_array]:\",x_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820bbef0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9b2f1b-8226-482b-83e1-5964cc212bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92cd451-8590-4d46-8461-7baa8c0399d9",
   "metadata": {},
   "source": [
    "# Build a PyTorch Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dbdbe1-7375-4c81-8abc-9b9847676997",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio  --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dc9d39-8057-4c0b-abf7-114e96ccff8e",
   "metadata": {},
   "source": [
    "# Prepare Multi-Step X_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfae2b3-3142-4c56-89c6-9c787efbe381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Suppose epoched_data has shape (N, channels, time) = (1000, 14, 128)\n",
    "# and y_array has shape (N,) = (1000,)\n",
    "\n",
    "N, channels, time = epoched_data.shape  # e.g. (1000, 14, 128)\n",
    "\n",
    "# We want the Transformer to see `seq_len = time` steps, each step is 'channels' features.\n",
    "# So final shape => (N, seq_len=128, input_dim=14).\n",
    "\n",
    "# Just transpose axis 1 and 2:\n",
    "X_seq = np.transpose(epoched_data, (0, 2, 1))  # => (N, 128, 14)\n",
    "\n",
    "print(\"X_seq shape:\", X_seq.shape)       # (1000, 128, 14)\n",
    "print(\"y_array shape:\", y_array.shape)   # (1000,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95938252-d0fc-4c1e-9cd0-6541b64563f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class EEGTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"\n",
    "        X shape: (num_samples, seq_len, input_dim)\n",
    "        y shape: (num_samples,)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        assert len(self.X) == len(self.y), \"Mismatch in samples\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # shape: (seq_len, input_dim)\n",
    "        x_item = self.X[idx]\n",
    "        y_item = self.y[idx]\n",
    "        # Convert to torch tensors\n",
    "        x_item = torch.tensor(x_item, dtype=torch.float32)\n",
    "        y_item = torch.tensor(y_item, dtype=torch.long)\n",
    "        return x_item, y_item\n",
    "\n",
    "# Create dataset\n",
    "dataset = EEGTimeSeriesDataset(X_seq, y_array)\n",
    "\n",
    "# Train/val/test split\n",
    "num_samples = len(dataset)\n",
    "indices = np.arange(num_samples)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_size = int(0.8 * num_samples)\n",
    "val_size   = int(0.1 * num_samples)\n",
    "test_size  = num_samples - train_size - val_size\n",
    "\n",
    "train_idx = indices[:train_size]\n",
    "val_idx   = indices[train_size : train_size+val_size]\n",
    "test_idx  = indices[train_size+val_size:]\n",
    "\n",
    "X_train, y_train = X_seq[train_idx], y_array[train_idx]\n",
    "X_val,   y_val   = X_seq[val_idx],   y_array[val_idx]\n",
    "X_test,  y_test  = X_seq[test_idx],  y_array[test_idx]\n",
    "\n",
    "train_ds = EEGTimeSeriesDataset(X_train, y_train)\n",
    "val_ds   = EEGTimeSeriesDataset(X_val,   y_val)\n",
    "test_ds  = EEGTimeSeriesDataset(X_test,  y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=16, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"Train size: {len(train_ds)} | Val size: {len(val_ds)} | Test size: {len(test_ds)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2bd004-dc22-4dbb-ac52-dbf56e015d81",
   "metadata": {},
   "source": [
    "Small Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a820a58c-50c3-4587-902c-3bbee5668a50",
   "metadata": {},
   "source": [
    "Here’s a light version of the code you saw earlier, but with logs. Note that seq_len=1 means we basically do a single “time step” pass, so the Transformer encoder is effectively operating on length 1. This is almost like an MLP—but it satisfies your professor’s requirement to use a “prediction model” (Transformer code path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b290d2-e030-4c98-8d14-470a3a87f686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # shape (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return x\n",
    "\n",
    "class SmallTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2, \n",
    "                 dim_feedforward=128, num_classes=20, dropout=0.1, pool=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.pool = pool\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, seq_len=128, input_dim=14)\n",
    "        returns: (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # 1) Embed\n",
    "        x_embed = self.embedding(x)             # => (batch_size, seq_len, d_model)\n",
    "        # 2) Positional Encoding\n",
    "        x_pe = self.pos_encoder(x_embed)        # => (batch_size, seq_len, d_model)\n",
    "        # 3) Transformer Encoder\n",
    "        enc_out = self.transformer_encoder(x_pe)# => (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # 4) Pool across time dimension\n",
    "        if self.pool == \"mean\":\n",
    "            pooled = enc_out.mean(dim=1)        # => (batch_size, d_model)\n",
    "        else:\n",
    "            pooled = enc_out[:, -1, :]         # => (batch_size, d_model)\n",
    "\n",
    "        # 5) Classify\n",
    "        logits = self.classifier(pooled)        # => (batch_size, num_classes)\n",
    "        return logits\n",
    "\n",
    "print(\"Multi-step Transformer code loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933ab8da-8d8f-45e7-a3f1-7387c55759dd",
   "metadata": {},
   "source": [
    "Training & Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcc1af6-7a92-4709-b92e-f8ba548334b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Set the device based on availability (MPS for Apple Silicon, CUDA for NVIDIA, else CPU)\n",
    "device = (\n",
    "    \"mps\" if torch.backends.mps.is_available() \n",
    "    else \"cuda\" if torch.cuda.is_available() \n",
    "    else \"cpu\"\n",
    ")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "num_classes = 20  # Adjust based on your dataset\n",
    "\n",
    "#########################################################\n",
    "# 1) Model, Criterion, Optimizer\n",
    "#########################################################\n",
    "model = SmallTransformer(\n",
    "    input_dim=14,  # e.g. 14 EEG channels\n",
    "    d_model=64,\n",
    "    nhead=4,\n",
    "    num_layers=2,\n",
    "    dim_feedforward=128,\n",
    "    num_classes=num_classes,\n",
    "    dropout=0.1,\n",
    "    pool=\"mean\"\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "#########################################################\n",
    "# 2) Training/Validation Setup\n",
    "#########################################################\n",
    "max_epochs = 100\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "val_f1_scores = []\n",
    "\n",
    "\n",
    "#########################################################\n",
    "# 3) Main Training Loop (No Early Stopping)\n",
    "#########################################################\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    #######################################################\n",
    "    # A) TRAINING\n",
    "    #######################################################\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_x)\n",
    "        loss = criterion(logits, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    #######################################################\n",
    "    # B) VALIDATION (just for monitoring, no early stop)\n",
    "    #######################################################\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_true  = []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            logits = model(batch_x)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_true.extend(batch_y.cpu().numpy())\n",
    "\n",
    "\n",
    "    # Compute validation accuracy and F1 score\n",
    "    acc = accuracy_score(val_true, val_preds)\n",
    "    f1 = f1_score(val_true, val_preds, average='weighted')  # or 'macro' depending on your needs\n",
    "    val_accuracies.append(acc)\n",
    "    val_f1_scores.append(f1)\n",
    "\n",
    "    # Print training info each epoch\n",
    "    print(f\"[Epoch {epoch}/{max_epochs}] Train Loss: {avg_train_loss:.4f} | Val Acc: {acc:.4f} | Val F1: {f1:.4f}\")\n",
    "\n",
    "#########################################################\n",
    "# 4) Evaluate on Test Set After Full 500 Epochs\n",
    "#########################################################\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_true  = []\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        logits = model(batch_x)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_true.extend(batch_y.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(test_true, test_preds)\n",
    "print(f\"\\nFinal Test Accuracy after {max_epochs} epochs: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b811056-f265-4ddd-82d8-9088accbe51e",
   "metadata": {},
   "source": [
    " Testing & Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fcde15-088d-48fb-859b-19c0075a2493",
   "metadata": {},
   "source": [
    "#  #Accuracy and F1-Score without PCA Transformer 83 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e12f35-bec1-4926-ba4d-f8d7cbdafabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Convert accuracies to percentages\n",
    "val_accuracies_percent = [acc * 100 for acc in val_accuracies]\n",
    "title = \"Transformer Relationship between Accuracy and F1-Score Without PCA\"\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(val_accuracies_percent, val_f1_scores, c=range(len(val_accuracies_percent)), cmap='viridis', marker='o')\n",
    "plt.xlabel(\"Validation Accuracy (%)\")\n",
    "plt.ylabel(\"Validation F1-Score\")\n",
    "plt.title(title)\n",
    "plt.grid(True)\n",
    "\n",
    "# Define the save path\n",
    "save_dir = \"transformer\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "file_name = title + \".png\"\n",
    "save_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "# Save the plot with higher resolution (dpi=300) and tight layout\n",
    "plt.savefig(save_path, dpi=900, bbox_inches='tight')\n",
    "# print(f\"Plot saved to {save_path}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Convert F1 scores to percentages (if needed for further processing)\n",
    "val_f1_scores_percent = [score * 100 for score in val_f1_scores]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6674cca2-62bc-4496-98b4-76a2a959c9f5",
   "metadata": {},
   "source": [
    "# Accuracy and F1-Score with PCA Transformer 100 Epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd06f64-613d-4e1c-96c4-238be6d9e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Convert accuracies to percentages\n",
    "val_accuracies_percent = [acc * 100 for acc in val_accuracies]\n",
    "title = \"Transformer Relationship between Accuracy and F1-Score With PCA\"\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(val_accuracies_percent, val_f1_scores, c=range(len(val_accuracies_percent)), cmap='viridis', marker='o')\n",
    "plt.xlabel(\"Validation Accuracy (%)\")\n",
    "plt.ylabel(\"Validation F1-Score\")\n",
    "plt.title(title)\n",
    "plt.grid(True)\n",
    "\n",
    "# Define the save path\n",
    "save_dir = \"transformer\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "file_name = title + \".png\"\n",
    "save_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "# Save the plot with high resolution before displaying\n",
    "plt.savefig(save_path, dpi=900, bbox_inches='tight')\n",
    "# print(f\"Plot saved to {save_path}\")\n",
    "\n",
    "# Now display the plot\n",
    "plt.show()\n",
    "\n",
    "# Convert F1 scores to percentages (if needed for further processing)\n",
    "val_f1_scores_percent = [score * 100 for score in val_f1_scores]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca2d1c6-e00b-4b31-95b5-1c6cf9c8ebc8",
   "metadata": {},
   "source": [
    "# Highest Accuracy & F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928ae97b-f623-467c-bf27-3b7ffa806361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming val_accuracies and val_f1_scores are lists with one entry per epoch\n",
    "best_epoch_index = np.argmax(val_accuracies)\n",
    "best_accuracy = val_accuracies[best_epoch_index] * 100  # converting to percentage for display\n",
    "best_f1 = val_f1_scores[best_epoch_index]  # keep F1 as a number\n",
    "\n",
    "print(\"Highest Accuracy: {:.2f}%\".format(best_accuracy))\n",
    "print(\"F1 Score corresponding to highest accuracy: {:.2f}\".format(best_f1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c85354-7679-4f2f-b511-f150d25f677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Convert accuracies to percentages\n",
    "val_accuracies_percent = [acc * 100 for acc in val_accuracies]\n",
    "title = \"Fig 4.2.2 CNN LSTM - Accuracy and F1-Score With PCA\"\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(val_accuracies_percent, val_f1_scores, c=range(len(val_accuracies_percent)), cmap='viridis', marker='o')\n",
    "plt.xlabel(\"Validation Accuracy (%)\")\n",
    "plt.ylabel(\"Validation F1-Score\")\n",
    "plt.title(title)\n",
    "plt.grid(True)\n",
    "\n",
    "# Define the save path\n",
    "save_dir = \"transformer\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "file_name = title + \".png\"\n",
    "save_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "# Save the plot with high resolution before displaying\n",
    "plt.savefig(save_path, dpi=900, bbox_inches='tight')\n",
    "# print(f\"Plot saved to {save_path}\")\n",
    "\n",
    "# Now display the plot\n",
    "plt.show()\n",
    "\n",
    "# Convert F1 scores to percentages (if needed for further processing)\n",
    "val_f1_scores_percent = [score * 100 for score in val_f1_scores]\n",
    "\n",
    "\n",
    "print(\"Highest Accuracy: {:.2f}%\".format(best_accuracy))\n",
    "print(\"F1 Score corresponding to highest accuracy: {:.2f}\".format(best_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0aac59-2496-47db-b444-c7a66084fa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Convert accuracies to percentages\n",
    "val_accuracies_percent = [acc * 100 for acc in val_accuracies]\n",
    "title = \"Fig 4.1.2 MultiLayer Perceptron - Accuracy and F1-Score With PCA\"\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(val_accuracies_percent, val_f1_scores, c=range(len(val_accuracies_percent)), cmap='viridis', marker='o')\n",
    "plt.xlabel(\"Validation Accuracy (%)\")\n",
    "plt.ylabel(\"Validation F1-Score\")\n",
    "plt.title(title)\n",
    "plt.grid(True)\n",
    "\n",
    "# Define the save path\n",
    "save_dir = \"transformer\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "file_name = title + \".png\"\n",
    "save_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "# Save the plot with high resolution before displaying\n",
    "plt.savefig(save_path, dpi=900, bbox_inches='tight')\n",
    "# print(f\"Plot saved to {save_path}\")\n",
    "\n",
    "# Now display the plot\n",
    "plt.show()\n",
    "\n",
    "# Convert F1 scores to percentages (if needed for further processing)\n",
    "val_f1_scores_percent = [score * 100 for score in val_f1_scores]\n",
    "\n",
    "\n",
    "print(\"Highest Accuracy: {:.2f}%\".format(best_accuracy))\n",
    "print(\"F1 Score corresponding to highest accuracy: {:.2f}\".format(best_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d523b92-c83d-4d06-9a30-95ac942de377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Convert accuracies to percentages\n",
    "val_accuracies_percent = [acc * 100 for acc in val_accuracies]\n",
    "title = \"Fig 4.3.2 MultiLayer Perceptron - Accuracy and F1-Score With PCA\"\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(val_accuracies_percent, val_f1_scores, c=range(len(val_accuracies_percent)), cmap='viridis', marker='o')\n",
    "plt.xlabel(\"Validation Accuracy (%)\")\n",
    "plt.ylabel(\"Validation F1-Score\")\n",
    "plt.title(title)\n",
    "plt.grid(True)\n",
    "\n",
    "# Define the save path\n",
    "save_dir = \"transformer\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "file_name = title + \".png\"\n",
    "save_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "# Save the plot with high resolution before displaying\n",
    "plt.savefig(save_path, dpi=900, bbox_inches='tight')\n",
    "# print(f\"Plot saved to {save_path}\")\n",
    "\n",
    "# Now display the plot\n",
    "plt.show()\n",
    "\n",
    "# Convert F1 scores to percentages (if needed for further processing)\n",
    "val_f1_scores_percent = [score * 100 for score in val_f1_scores]\n",
    "\n",
    "\n",
    "print(\"Highest Accuracy: {:.2f}%\".format(best_accuracy))\n",
    "print(\"F1 Score corresponding to highest accuracy: {:.2f}\".format(best_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44b2dc9-dfb7-464d-90b9-ff13d15bdc23",
   "metadata": {},
   "source": [
    "# Plotting Per-Word Performance (Bag of 20 Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82efed6f-be63-4b2f-84bd-899ea5bc38e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: num_classes = 20 and label2word mapping available\n",
    "num_classes = 20\n",
    "per_class_accuracy = []\n",
    "per_class_f1 = []\n",
    "\n",
    "# Convert to numpy arrays if not already\n",
    "all_true = np.array(all_true_labels)\n",
    "all_pred = np.array(all_predictions)\n",
    "\n",
    "for i in range(num_classes):\n",
    "    idx = np.where(all_true == i)[0]\n",
    "    if len(idx) == 0:\n",
    "        per_class_accuracy.append(0)\n",
    "        per_class_f1.append(0)\n",
    "    else:\n",
    "        acc = accuracy_score(all_true[idx], all_pred[idx])\n",
    "        f1 = f1_score(all_true[idx], all_pred[idx], average='weighted')\n",
    "        per_class_accuracy.append(acc)\n",
    "        per_class_f1.append(f1)\n",
    "\n",
    "# Get word labels (ensure order corresponds to class indices)\n",
    "words = [label2word[i] for i in range(num_classes)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.bar(words, per_class_accuracy, alpha=0.6, label='Accuracy')\n",
    "ax.plot(words, per_class_f1, color='red', marker='o', label='F1 Score')\n",
    "ax.set_xlabel(\"Words\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Per-Word Performance (Accuracy & F1 Score)\")\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288de150-a5a7-438b-a039-9be1dea3b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "\n",
    "# ---------------------------\n",
    "# Sample Data Generation\n",
    "# ---------------------------\n",
    "# Replace this with your actual data matrix X (shape: [n_samples, n_features])\n",
    "# For demonstration, we create random data with 1000 samples and 50 features.\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 50)\n",
    "\n",
    "# ---------------------------\n",
    "# Perform PCA\n",
    "# ---------------------------\n",
    "# Fit PCA on the data (you can set n_components=None to compute all components)\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Get explained variance ratios\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# ---------------------------\n",
    "# Plot 1: Scree Plot\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "components = np.arange(1, len(explained_variance) + 1)\n",
    "plt.plot(components, explained_variance, marker='o', linestyle='--', color='b')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Scree Plot: Explained Variance by Principal Components')\n",
    "plt.xticks(components)\n",
    "plt.grid(True)\n",
    "\n",
    "# Save scree plot\n",
    "save_dir = \"transformer\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "scree_plot_path = os.path.join(save_dir, \"PCA_Scree_Plot.png\")\n",
    "plt.savefig(scree_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Scree plot saved to {scree_plot_path}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Plot 2: Scatter Plot of the First Two Principal Components\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.7, edgecolor='k')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Visualization of Feature Reduction using PCA')\n",
    "plt.grid(True)\n",
    "\n",
    "# Save scatter plot\n",
    "scatter_plot_path = os.path.join(save_dir, \"PCA_Scatter_Plot.png\")\n",
    "plt.savefig(scatter_plot_path, dpi=600, bbox_inches='tight')\n",
    "print(f\"Scatter plot saved to {scatter_plot_path}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fac454-a10e-4ad3-99a6-81c7efceaab2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_true = []\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        logits = model(batch_x)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_true.extend(batch_y.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(test_true, test_preds)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "cm = confusion_matrix(test_true, test_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(num_classes))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "cm = confusion_matrix(test_true, test_preds)\n",
    "# label2word = {v: k for k, v in num_dict.items()}\n",
    "label2word = num_dict\n",
    "num_classes = len(label2word)\n",
    "labels_for_cm = [label2word[i] for i in range(num_classes)]\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_for_cm)\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, xticks_rotation='vertical')\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ff935c-5cde-47c8-9ebf-996c97231218",
   "metadata": {},
   "source": [
    "# Retrieve Predicted Labels for Each Sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00746d-5e92-40ea-8393-2672bd797f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "all_sample_indices = []  # optional: track which sample index\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (batch_x, batch_y) in enumerate(test_loader):\n",
    "        # batch_x shape: (batch_size, seq_len, input_dim)\n",
    "        # batch_y shape: (batch_size,)\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(batch_x)  # => shape (batch_size, num_classes)\n",
    "        preds = logits.argmax(dim=-1)  # => shape (batch_size,)\n",
    "\n",
    "        # Convert to CPU numpy arrays\n",
    "        preds_np = preds.cpu().numpy()\n",
    "        labels_np = batch_y.cpu().numpy()\n",
    "\n",
    "        # Store results\n",
    "        all_predictions.extend(preds_np)\n",
    "        all_true_labels.extend(labels_np)\n",
    "\n",
    "        # If you want to track the test sample index:\n",
    "        start_idx = batch_idx * test_loader.batch_size\n",
    "        sample_indices = range(start_idx, start_idx + len(batch_y))\n",
    "        all_sample_indices.extend(sample_indices)\n",
    "\n",
    "# Now all_predictions[i] corresponds to the predicted label for sample i\n",
    "# all_true_labels[i] is the ground-truth label for sample i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46631663-4341-40b1-8ed2-46d7bb93d331",
   "metadata": {},
   "source": [
    "# Mapping Labels Back to Words (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828ae366-90cb-4bba-bb5d-e1e342bf6201",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2word = {v: k for k, v in num_dict.items()}\n",
    "\n",
    "# label2word = num_dict\n",
    "\n",
    "for i in range(len(all_predictions)):\n",
    "    pred_label = all_predictions[i]\n",
    "    true_label = all_true_labels[i]\n",
    "    pred_word = label2word[pred_label]\n",
    "    true_word = label2word[true_label]\n",
    "    # print(f\"Sample {i}: Predicted '{pred_word}', Actual '{true_word}'\")\n",
    "\n",
    "correct_predictions = sum(1 for i in range(len(all_predictions)) if all_predictions[i] == all_true_labels[i])\n",
    "total_predictions = len(all_predictions)\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Total Correct: {correct_predictions}/{total_predictions} ({accuracy * 100:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63433392-f0de-49be-b605-edd99e0c2332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Model Accuracy and F1 Score Over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678e42ed-0887-4def-9785-0112bece3449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume these arrays exist from your training loop\n",
    "# val_accuracies = [ ... ]  # list of validation accuracies per epoch\n",
    "# val_f1_scores = [ ... ]   # list of validation F1 scores per epoch\n",
    "\n",
    "epochs = range(1, len(val_accuracies) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, val_accuracies, label='Validation Accuracy', marker='o')\n",
    "plt.plot(epochs, val_f1_scores, label='Validation F1 Score', marker='o')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Model Accuracy and F1 Score Over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125b923e-b7a0-4b0f-be1a-a63ff88206a1",
   "metadata": {},
   "source": [
    "# Storing Results to a CSV or TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a989a4-0a16-44c7-8b84-5d0ab328b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"predictions.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"SampleIndex\", \"PredLabel\", \"TrueLabel\", \"PredWord\", \"TrueWord\"])\n",
    "\n",
    "    for i in range(len(all_predictions)):\n",
    "        pred_label = all_predictions[i]\n",
    "        true_label = all_true_labels[i]\n",
    "        pred_word = label2word[pred_label]\n",
    "        true_word = label2word[true_label]\n",
    "\n",
    "        # If you tracked sample indices:\n",
    "        sample_idx = all_sample_indices[i]\n",
    "        writer.writerow([sample_idx, pred_label, true_label, pred_word, true_word])\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
